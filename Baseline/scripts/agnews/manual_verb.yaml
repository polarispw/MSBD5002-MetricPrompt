# task config
task: classification

classification:
  auto_t: false
  auto_v: false
  parent_config: task
  loss_function: cross_entropy
  metric:
      - accuracy

# environment config
environment:
  num_gpus: 1
  cuda_visible_devices:
    - 0
  local_rank: 0

logging:
  console_level: INFO
  datetime_format: '%m%d%H%M%S%f'
  file_level: NOTSET
  overwrite: true
  path_base: logs
  unique_string_keys:
  - dataset.name
  - plm.model_path
  - verbalizer
  - datetime

# dataset config
dataset:
  name: agnews

dataloader:
  max_seq_length: 512

# model config
plm:
  model_name: bert
  model_path: bert-base-uncased
  optimize:
    freeze_para: False
    lr: 1.0e-05
    weight_decay: 0.01
    no_decay:
      - bias
      - LayerNorm.weight
    scheduler:
      type: null
      num_warmup_steps: 500

# training config
checkpoint:
  higher_better: true
  save_best: true
  save_latest: false

train:
  batch_size: 2
  num_epochs: 5

test:
  batch_size: 32

dev:
  batch_size: 32

# prompt config
template: manual_template
verbalizer: manual_verbalizer

manual_template:
  # for soft-verb or dbpedia choose 1
  # for yahoo but not soft-verb choose 2
  choice: 0
  file_path: scripts/manual_template.txt

manual_verbalizer:
  choice: 0
  file_path: scripts/manual_verbalizer_agnews.txt

# learning settings
learning_setting: few_shot

few_shot:
  parent_config: learning_setting
  few_shot_sampling: sampling_from_train
  
sampling_from_train:
  parent_config: few_shot_sampling
  num_examples_per_label: 2
  also_sample_dev: True
  num_examples_per_label_dev: 8
  seed:
    - 123
